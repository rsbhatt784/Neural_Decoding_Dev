{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cross-Bucket Tests on all Acc/Dec and Random Fragments Kalman Filter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part A: (1) Import Fragment Data from MATLAB, (2) Save Them in a .pickle file, and (3) Load Them In\n",
    "\n",
    "###### (HOWEVER, YOU DON'T NEED TO LOAD THEM IN AGAIN SINCE STEP 1 ALREADY DOES THAT FOR YOU!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "## (Configuration) Allows you to return multiple variables from a single cell ##\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = 'all'\n",
    "\n",
    "## Allows you to import files from another folder in current directory ## \n",
    "import os \n",
    "import sys \n",
    "module_path = os.path.abspath(os.path.join('..'))\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import standard packages\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "from scipy import io\n",
    "from scipy import stats\n",
    "import pickle\n",
    "import sys\n",
    "\n",
    "#Import metrics\n",
    "from Neural_Decoding.metrics import get_R2\n",
    "from Neural_Decoding.metrics import get_rho\n",
    "from Neural_Decoding.metrics import get_R2_parts\n",
    "\n",
    "#Import decoder functions\n",
    "from Neural_Decoding.decoders import KalmanFilterDecoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part B: Preprocessing Decoder Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Import Data ##\n",
    "# We'll load in position data and derive velocity and acceleration from it \n",
    "\n",
    "# Specify Fragment Types to be used for this anaylsis \n",
    "frag_type = ['AD', 'Rand'] \n",
    "# Specify folder where MATLAB data is stored\n",
    "folder = '/Users/rbhatt6/Documents/MATLAB/' \n",
    "\n",
    " #locals()[\"sortIn\"+frag_type[i]] = io.loadmat(folder+'cleanedSortIn.mat')\n",
    "for i in range(len(frag_type)):\n",
    "    input = \"sortIn_\"+frag_type[i]\n",
    "    locals()[input] = io.loadmat(folder + input + '.mat')\n",
    "\n",
    "    output = \"sortOut_\" + frag_type[i]\n",
    "    locals()[output] = io.loadmat(folder + output + '.mat')\n",
    "\n",
    "    locals()[input] = np.squeeze(list(locals()[input].values())[3])\n",
    "    locals()[output] = np.squeeze(list(locals()[output].values())[3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Format Kinematics Data (outputs) ##\n",
    "# For the Kalman filter, we use the position, velocity, and acceleration as outputs.\n",
    "# Ultimately, we are only concerned with the goodness of fit of velocity, but using them all as covariates helps performance.\n",
    "\n",
    "for i in range(len(frag_type)):\n",
    "    # Pulling local variables into new, temp variables\n",
    "    output = locals()[\"sortOut_\"+frag_type[i]]\n",
    "    \n",
    "    # Creating a local variable for each decoder_output\n",
    "    decoder_output = \"decoder_output_\" + frag_type[i]\n",
    "    locals()[decoder_output] = []\n",
    "\n",
    "    for j in range(len(output)): # Number of buckets (i.e. 16 or 8)\n",
    "        nFrags = output[j][0].shape[0]\n",
    "        temp2 = []\n",
    "        for k in range(0, nFrags, 1): # Number of fragments #output[0][0].shape[0]\n",
    "            vel_X = float(output[j][0][k])\n",
    "            vel_Y = float(output[j][1][k])\n",
    "            acc_X = float(output[j][2][k])\n",
    "            acc_Y = float(output[j][3][k])\n",
    "            pos_X = float(output[j][4][k])\n",
    "            pos_Y = float(output[j][5][k])\n",
    "            temp = [vel_X, vel_Y, acc_X, acc_Y, pos_X, pos_Y]\n",
    "            temp2.append(np.array(temp))\n",
    "            #locals()[decoder_output][j][k].append(np.array(temp))\n",
    "        #locals()[decoder_output][j].append(np.array(temp2))\n",
    "        locals()[decoder_output].append(np.array(temp2))\n",
    "        #temp = np.array(np.concatenate((vel_X, vel_Y, acc_X, acc_Y, outputX[j], outputY[j]),axis=1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part C: Partitioning and Running the Kalman Filter on the Same Buckets "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# (NEW) CROSS-VALIDATED WITHIN-BUCKET TEST (for all 16 buckets)\n",
    "# Doing a 10-fold cross validation procedure\n",
    "\n",
    "# for loop through 10 folds then 16 buckets\n",
    "\n",
    "for i in range(len(frag_type)):\n",
    "\n",
    "   # Pulling local variables \n",
    "   input = locals()[\"sortIn_\"+frag_type[i]]\n",
    "   output = locals()[\"decoder_output_\"+frag_type[i]]\n",
    "\n",
    "   # Creating a local variable to hold final training and testing data \n",
    "   final_X_train = \"final_X_train_\" + frag_type[i]\n",
    "   final_y_train = \"final_y_train_\" + frag_type[i]\n",
    "   final_X_test = \"final_X_test_\" + frag_type[i]\n",
    "   final_y_test = \"final_y_test_\" + frag_type[i]\n",
    "   locals()[final_X_train] = []\n",
    "   locals()[final_y_train] = []\n",
    "   locals()[final_X_test] = []\n",
    "   locals()[final_y_test] = []\n",
    "\n",
    "   idx_list = [0,1,2,3,4,5,6,7,8,9]\n",
    "\n",
    "   # Splitting data for a single bucket into 10 folds - (j) is the left-out fold for testing\n",
    "   for j in range(0,10):\n",
    "      test_idx = j\n",
    "      train_idx = idx_list.copy()\n",
    "      train_idx.pop(j)\n",
    "\n",
    "      temp_X_train = []\n",
    "      temp_y_train = []\n",
    "      temp_X_test = []\n",
    "      temp_y_test = []\n",
    "      \n",
    "      for k in range(len(input)):\n",
    "         X_split = np.array_split(input[k], 10)\n",
    "         y_split = np.array_split(output[k], 10)\n",
    "\n",
    "         # Index the list of indices to create a list of np ndarrays for training (9 arrays) and testing (1 array)\n",
    "         X_train = [X_split[i] for i in train_idx]\n",
    "         X_test = X_split[j]\n",
    "         y_train = [y_split[i] for i in train_idx]\n",
    "         y_test = y_split[j]\n",
    "\n",
    "         # Concatenate the list of 9 arrays for the training sets \n",
    "         X_train = np.concatenate(X_train, axis=0)\n",
    "         y_train = np.concatenate(y_train, axis=0)\n",
    "         \n",
    "         temp_X_train.append(X_train)\n",
    "         temp_y_train.append(y_train)\n",
    "         temp_X_test.append(X_test)\n",
    "         temp_y_test.append(y_test)\n",
    "\n",
    "      locals()[final_X_train].append(temp_X_train)\n",
    "      locals()[final_y_train].append(temp_y_train)\n",
    "      locals()[final_X_test].append(temp_X_test)\n",
    "      locals()[final_y_test].append(temp_y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check for Decoder Accuracy\n",
    "### Training on 15 buckets and Testing on 1 bucket instead"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Within-Bucket Test with 10-fold Cross-Validation \n",
    "from Neural_Decoding.runModelsKF import run_model_kf_test\n",
    "\n",
    "for i in range(len(frag_type)):\n",
    "\n",
    "    # Pulling local variables \n",
    "    frag_X_train =  locals()[\"final_X_train_\" + frag_type[i]]\n",
    "    frag_y_train = locals()[\"final_y_train_\" + frag_type[i]]\n",
    "    frag_X_test = locals()[\"final_X_test_\" + frag_type[i]]\n",
    "    frag_y_test = locals()[\"final_y_test_\" + frag_type[i]]\n",
    "\n",
    "    # Creating a local variable to hold predicted outputs for each frag type AND their trained models\n",
    "    parts = \"pred_parts_\" + frag_type[i]\n",
    "    models = \"more_trained_models_\" + frag_type[i]\n",
    "    locals()[parts] = []\n",
    "    locals()[models] = []\n",
    "\n",
    "    for cross in range(0,10):\n",
    "        X_train = frag_X_train[cross]\n",
    "        y_train = frag_y_train[cross]\n",
    "        X_test = frag_X_test[cross]\n",
    "        y_test = frag_y_test[cross]\n",
    "        \n",
    "        curr_R2s, curr_models = run_model_kf_test(X_train, y_train, X_test,y_test, \"parts\")\n",
    "        locals()[parts].append(curr_R2s)\n",
    "        locals()[models].append(curr_models)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute Within-Bucket combined XY_FVAF \n",
    "# For velocity only, although all kinematic variables should be used for model-fitting and predicted for.\n",
    "from Neural_Decoding.metrics import compute_XY_FVAF\n",
    "\n",
    "# 'pred_parts_AD' was thankfully also a list of arrays containing the R2s\n",
    "\n",
    "for i in range(len(frag_type)):\n",
    "\n",
    "    # Pulling local variables (computed R2s for parts)\n",
    "    R2s_parts = locals()[\"pred_parts_\" + frag_type[i]]\n",
    "\n",
    "    # Creating a local variable to hold final XY_FVAFs\n",
    "    XY_FVAFs = \"XY_FVAFs_\" + frag_type[i]\n",
    "    locals()[XY_FVAFs] = []\n",
    "\n",
    "    for j in range(0,10):\n",
    "        curr_fold = R2s_parts[j]\n",
    "\n",
    "        curr_fold_XY_FVAFs = []\n",
    "        for k in range(0,16):\n",
    "            #curr_bucket = Kalman_R2s_combined[i]\n",
    "            vel_x_nom = curr_fold[k][0][0] # dim = (curr_bucket, nom, x_vel)\n",
    "            vel_x_denom = curr_fold[k][1][0] # dim = (curr_bucket, denom, x_vel)\n",
    "            vel_y_nom = curr_fold[k][0][1] # dim = (curr_bucket, nom, y_vel)\n",
    "            vel_y_denom = curr_fold[k][1][1] # dim = (curr_bucket, denom, y_vel)\n",
    "\n",
    "            XY_FVAF = compute_XY_FVAF(vel_x_nom,vel_x_denom,vel_y_nom,vel_y_denom)\n",
    "            curr_fold_XY_FVAFs.append(XY_FVAF)\n",
    "\n",
    "        locals()[XY_FVAFs].append(curr_fold_XY_FVAFs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[-1.4571230620931397,\n",
       "  -1.1846134048645154,\n",
       "  -0.8207205862457452,\n",
       "  -0.814745116907047,\n",
       "  -1.292928409702053,\n",
       "  -0.598931898853164,\n",
       "  -0.5927502258960964,\n",
       "  -0.834487843214043,\n",
       "  -1.502806437404249,\n",
       "  -1.1421879968808892,\n",
       "  -0.6072785659945226,\n",
       "  -1.6211931176767784,\n",
       "  -1.5300251076664293,\n",
       "  -1.4794785913879176,\n",
       "  -0.39013945411981954,\n",
       "  -0.8857477211320386],\n",
       " [-2.840569765698738,\n",
       "  -1.3167112352173604,\n",
       "  -0.6732968194618283,\n",
       "  -0.927594528375902,\n",
       "  -0.9313015474525199,\n",
       "  -1.113326374106212,\n",
       "  -0.4386858201102286,\n",
       "  -0.8898030129232035,\n",
       "  -1.4970654531485859,\n",
       "  -0.9112670683195472,\n",
       "  -0.6500987693683551,\n",
       "  -0.9076036697229148,\n",
       "  -1.321889866728172,\n",
       "  -1.1334468033302576,\n",
       "  -0.7496156981400901,\n",
       "  -1.3791216005316809],\n",
       " [-1.5317783045048774,\n",
       "  -1.172033436794972,\n",
       "  -0.5003569383156901,\n",
       "  -0.9404624349808788,\n",
       "  -1.6798137482270588,\n",
       "  -0.80311275561573,\n",
       "  -0.6262415431837096,\n",
       "  -1.0223720033166095,\n",
       "  -1.469289952539524,\n",
       "  -1.0029809889545436,\n",
       "  -0.4777713461757569,\n",
       "  -1.0425264466772841,\n",
       "  -3.52568809746069,\n",
       "  -1.4996816496724663,\n",
       "  -0.5077776654801547,\n",
       "  -1.1944217886171926],\n",
       " [-1.8538922052649505,\n",
       "  -1.2608321220954393,\n",
       "  -0.5486831581406888,\n",
       "  -0.6928987395138326,\n",
       "  -1.0879640766495937,\n",
       "  -1.0977665416872617,\n",
       "  -0.6005978599266892,\n",
       "  -0.9466435903121577,\n",
       "  -1.2405491394540302,\n",
       "  -0.9407944751778774,\n",
       "  -0.7933048176878046,\n",
       "  -0.9291779355684056,\n",
       "  -1.8450157011111243,\n",
       "  -1.6219432652165682,\n",
       "  -0.5230359133748665,\n",
       "  -0.786379716864714],\n",
       " [-1.323489084563001,\n",
       "  -0.9702373977737289,\n",
       "  -0.7246895316413242,\n",
       "  -0.7884821588414173,\n",
       "  -1.1920438715711925,\n",
       "  -1.2371172477340915,\n",
       "  -0.5164094264388237,\n",
       "  -1.284132757996066,\n",
       "  -1.1938186999846607,\n",
       "  -0.7195810528840634,\n",
       "  -1.054609878205528,\n",
       "  -0.8385686017868024,\n",
       "  -0.9185629986961528,\n",
       "  -1.0885204124723251,\n",
       "  -0.4276532287226418,\n",
       "  -1.0445354008048953],\n",
       " [-1.473091563695701,\n",
       "  -0.8777427182539626,\n",
       "  -0.6168726881354505,\n",
       "  -0.8605732042811451,\n",
       "  -2.1428349369396136,\n",
       "  -0.8064923425732127,\n",
       "  -0.5184142387868973,\n",
       "  -1.1429088517627783,\n",
       "  -1.1300832614239535,\n",
       "  -1.2953158815444943,\n",
       "  -0.9553756637323252,\n",
       "  -1.2081514886577152,\n",
       "  -1.6650129776493197,\n",
       "  -1.737750464289499,\n",
       "  -0.505570448589411,\n",
       "  -1.067251928489779],\n",
       " [-1.2665478847790559,\n",
       "  -0.9412754218925143,\n",
       "  -0.8043925034307011,\n",
       "  -0.5960878196248605,\n",
       "  -1.4739759958596204,\n",
       "  -0.8731783167272629,\n",
       "  -0.5760605205011404,\n",
       "  -0.7716658547081392,\n",
       "  -1.1178279350665377,\n",
       "  -0.9814051256961207,\n",
       "  -0.559578497583771,\n",
       "  -1.4251260171956028,\n",
       "  -1.95629465587534,\n",
       "  -1.136082948284038,\n",
       "  -0.7235630547442364,\n",
       "  -0.8180895066822158],\n",
       " [-0.8420379459557574,\n",
       "  -1.253887034525086,\n",
       "  -0.8690483632825432,\n",
       "  -0.8562773787041451,\n",
       "  -1.387576257213833,\n",
       "  -0.7193607797764188,\n",
       "  -0.6944989197119602,\n",
       "  -0.8721729033985588,\n",
       "  -1.7944274798656479,\n",
       "  -1.1431723421969093,\n",
       "  -0.6587294388105747,\n",
       "  -1.2622633268299972,\n",
       "  -2.0117565372899797,\n",
       "  -1.3459895730474232,\n",
       "  -0.4952105526047972,\n",
       "  -1.2484262951686893],\n",
       " [-2.298550477770816,\n",
       "  -0.9183643400751458,\n",
       "  -0.5523491441654562,\n",
       "  -0.9432989034686539,\n",
       "  -1.3875571211271414,\n",
       "  -1.1363854970320428,\n",
       "  -0.517693027701478,\n",
       "  -1.3466323491789796,\n",
       "  -1.1426862603767365,\n",
       "  -1.3338367141166523,\n",
       "  -0.5793689732508058,\n",
       "  -1.358564006314952,\n",
       "  -2.3493921370856823,\n",
       "  -1.767791733338992,\n",
       "  -0.6117569193043706,\n",
       "  -1.2670001962287571],\n",
       " [-1.2954066930556047,\n",
       "  -1.0719053055461614,\n",
       "  -0.8492920787299822,\n",
       "  -0.9744240193306049,\n",
       "  -1.2208200829784044,\n",
       "  -0.8720292375634529,\n",
       "  -0.6215226076082954,\n",
       "  -1.1571407736008221,\n",
       "  -1.5314622059478817,\n",
       "  -1.0326730473471053,\n",
       "  -0.3460599590028215,\n",
       "  -1.7327547870657432,\n",
       "  -1.3065772709242225,\n",
       "  -1.2490933339424468,\n",
       "  -0.583221576135472,\n",
       "  -1.1652230685205858]]"
      ]
     },
     "execution_count": 179,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "XY_FVAFs_AD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving results in a matlab file \n",
    "from scipy.io import savemat\n",
    "\n",
    "result1 = [XY_FVAFs_AD, XY_FVAFs_Rand]\n",
    "FrameStack1 = np.empty((2,), dtype=object)\n",
    "for i in range(len(result1)):\n",
    "    FrameStack1[i] = result1[i]\n",
    "\n",
    "#savemat(\"Acoss_All_Buckets_100neurons.mat\", {\"Across_All_Buckets_100_neurons\":FrameStack1})\n",
    "savemat(\"Acoss_All_Buckets_89neurons.mat\", {\"Across_All_Buckets_89_neurons\":FrameStack1})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training and Testing Models for Same Direction Or Same Polarity Only "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Direction-Specific Test\n",
    "from Neural_Decoding.runModelsKF import run_model_kf_direction\n",
    "\n",
    "for i in range(len(frag_type)):\n",
    "\n",
    "    # Pulling local variables \n",
    "    frag_X_train =  locals()[\"final_X_train_\" + frag_type[i]]\n",
    "    frag_y_train = locals()[\"final_y_train_\" + frag_type[i]]\n",
    "    frag_X_test = locals()[\"final_X_test_\" + frag_type[i]]\n",
    "    frag_y_test = locals()[\"final_y_test_\" + frag_type[i]]\n",
    "\n",
    "    # Creating a local variable to hold predicted outputs for each frag type AND their trained models\n",
    "    parts = \"pred_parts_\" + frag_type[i]\n",
    "    #models = \"direction_specific_models_\" + frag_type[i]\n",
    "    locals()[parts] = []\n",
    "    #locals()[models] = []\n",
    "\n",
    "    for cross in range(0,10):\n",
    "        X_train = frag_X_train[cross]\n",
    "        y_train = frag_y_train[cross]\n",
    "        X_test = frag_X_test[cross]\n",
    "        y_test = frag_y_test[cross]\n",
    "        \n",
    "        curr_R2s, curr_models = run_model_kf_direction(X_train, y_train, X_test,y_test, \"parts\")\n",
    "        locals()[parts].append(curr_R2s)\n",
    "        #locals()[models].append(curr_models)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Neural_Decoding.metrics import compute_XY_FVAF\n",
    "\n",
    "for i in range(len(frag_type)):\n",
    "\n",
    "    # Pulling local variables (computed R2s for parts)\n",
    "    R2s_parts = locals()[\"pred_parts_\" + frag_type[i]]\n",
    "\n",
    "    # Creating a local variable to hold final XY_FVAFs\n",
    "    XY_FVAFs = \"direction_specific_XY_FVAFs_\" + frag_type[i]\n",
    "    locals()[XY_FVAFs] = []\n",
    "\n",
    "    for j in range(0,10):\n",
    "        curr_fold = R2s_parts[j]\n",
    "\n",
    "        curr_fold_XY_FVAFs = []\n",
    "        for k in range(0,8):\n",
    "            #curr_bucket = Kalman_R2s_combined[i]\n",
    "            vel_x_nom = curr_fold[k][0][0] # dim = (curr_bucket, nom, x_vel)\n",
    "            vel_x_denom = curr_fold[k][1][0] # dim = (curr_bucket, denom, x_vel)\n",
    "            vel_y_nom = curr_fold[k][0][1] # dim = (curr_bucket, nom, y_vel)\n",
    "            vel_y_denom = curr_fold[k][1][1] # dim = (curr_bucket, denom, y_vel)\n",
    "\n",
    "            XY_FVAF = compute_XY_FVAF(vel_x_nom,vel_x_denom,vel_y_nom,vel_y_denom)\n",
    "            curr_fold_XY_FVAFs.append(XY_FVAF)\n",
    "\n",
    "        locals()[XY_FVAFs].append(curr_fold_XY_FVAFs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[0.21695834845165074,\n",
       "  0.09371795094109614,\n",
       "  0.1595158364193231,\n",
       "  0.17227110237801224,\n",
       "  0.10432952212691482,\n",
       "  0.08254187381287825,\n",
       "  0.20121796818156856,\n",
       "  0.12821236588067386],\n",
       " [0.21197082686873225,\n",
       "  0.18204884935510546,\n",
       "  0.15526313578604434,\n",
       "  0.2081088678088292,\n",
       "  0.16825448225980955,\n",
       "  0.0416625651442144,\n",
       "  0.1358824459805137,\n",
       "  0.1873656836627],\n",
       " [0.12718850455387376,\n",
       "  0.08789548362589195,\n",
       "  0.13981736118568855,\n",
       "  0.15133551650771015,\n",
       "  0.13739949590080003,\n",
       "  0.13971239311451888,\n",
       "  0.17916572506215833,\n",
       "  0.1280446694911267],\n",
       " [0.1672258426423291,\n",
       "  0.13462951614332808,\n",
       "  0.20758468331779167,\n",
       "  0.034945980578425795,\n",
       "  0.07663979887104888,\n",
       "  0.09125737684755142,\n",
       "  0.11301868465568798,\n",
       "  0.16545115987942927],\n",
       " [0.21728167770054796,\n",
       "  0.20620866523844794,\n",
       "  0.163943287310377,\n",
       "  0.08577363463589849,\n",
       "  0.1155274949766173,\n",
       "  0.1536971425780448,\n",
       "  0.16624825752540606,\n",
       "  0.11977142432401111],\n",
       " [0.0970237655253372,\n",
       "  0.1685561730062689,\n",
       "  0.1846543902299972,\n",
       "  0.1354225654897454,\n",
       "  0.23809591795871732,\n",
       "  0.13384514550985338,\n",
       "  0.13842032837727303,\n",
       "  0.18730539769542842],\n",
       " [0.14112782010725233,\n",
       "  0.12806342734414922,\n",
       "  0.08703011020587759,\n",
       "  0.05585173488828987,\n",
       "  0.16773795922676382,\n",
       "  0.12320635355732001,\n",
       "  0.119006762166042,\n",
       "  0.14268241438174434],\n",
       " [0.2240257546197142,\n",
       "  0.09608627795044877,\n",
       "  0.11779458312933377,\n",
       "  0.17843758328988146,\n",
       "  0.17374997513171342,\n",
       "  0.13179091833126189,\n",
       "  0.20126864528901622,\n",
       "  0.15948421078498054],\n",
       " [0.17778329722394626,\n",
       "  0.17173095258023463,\n",
       "  0.15823335199762034,\n",
       "  0.14115297551803296,\n",
       "  0.12389983354337475,\n",
       "  0.13796291662656324,\n",
       "  0.12502718640579502,\n",
       "  0.19028590005041812],\n",
       " [0.22466704819487848,\n",
       "  0.1354217622384738,\n",
       "  0.10377455116344347,\n",
       "  0.17260268587250738,\n",
       "  0.17125082732344854,\n",
       "  0.2132643197671794,\n",
       "  0.1266788049895512,\n",
       "  0.15775912493908928]]"
      ]
     },
     "execution_count": 251,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "direction_specific_XY_FVAFs_Rand"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving results in a matlab file \n",
    "from scipy.io import savemat\n",
    "\n",
    "result1 = [direction_specific_XY_FVAFs_AD, direction_specific_XY_FVAFs_Rand]\n",
    "\n",
    "FrameStack1 = np.empty((2,), dtype=object)\n",
    "\n",
    "for i in range(len(result1)):\n",
    "    FrameStack1[i] = result1[i]\n",
    "savemat(\"Direction_Specific_Buckets_3.mat\", {\"Direction_Specific_XY_FVAFs_3\":FrameStack1})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Polarity-Specific Test\n",
    "from Neural_Decoding.runModelsKF import run_model_kf_polarity\n",
    "\n",
    "for i in range(len(frag_type)):\n",
    "\n",
    "    # Pulling local variables \n",
    "    frag_X_train =  locals()[\"final_X_train_\" + frag_type[i]]\n",
    "    frag_y_train = locals()[\"final_y_train_\" + frag_type[i]]\n",
    "    frag_X_test = locals()[\"final_X_test_\" + frag_type[i]]\n",
    "    frag_y_test = locals()[\"final_y_test_\" + frag_type[i]]\n",
    "\n",
    "    # Creating a local variable to hold predicted outputs for each frag type AND their trained models\n",
    "    parts = \"pred_parts_\" + frag_type[i]\n",
    "    models = \"polarity_specific_models_\" + frag_type[i]\n",
    "    locals()[parts] = []\n",
    "    locals()[models] = []\n",
    "\n",
    "    for cross in range(0,10):\n",
    "        X_train = frag_X_train[cross]\n",
    "        y_train = frag_y_train[cross]\n",
    "        X_test = frag_X_test[cross]\n",
    "        y_test = frag_y_test[cross]\n",
    "        \n",
    "        curr_R2s, curr_models = run_model_kf_polarity(X_train, y_train, X_test,y_test, \"parts\")\n",
    "        locals()[parts].append(curr_R2s)\n",
    "        locals()[models].append(curr_models)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Neural_Decoding.metrics import compute_XY_FVAF\n",
    "\n",
    "for i in range(len(frag_type)):\n",
    "\n",
    "    # Pulling local variables (computed R2s for parts)\n",
    "    R2s_parts = locals()[\"pred_parts_\" + frag_type[i]]\n",
    "\n",
    "    # Creating a local variable to hold final XY_FVAFs\n",
    "    XY_FVAFs = \"polarity_specific_XY_FVAFs_\" + frag_type[i]\n",
    "    locals()[XY_FVAFs] = []\n",
    "\n",
    "    for j in range(0,10):\n",
    "        curr_fold = R2s_parts[j]\n",
    "\n",
    "        curr_fold_XY_FVAFs = []\n",
    "        for k in range(0,2):\n",
    "            #curr_bucket = Kalman_R2s_combined[i]\n",
    "            vel_x_nom = curr_fold[k][0][0] # dim = (curr_bucket, nom, x_vel)\n",
    "            vel_x_denom = curr_fold[k][1][0] # dim = (curr_bucket, denom, x_vel)\n",
    "            vel_y_nom = curr_fold[k][0][1] # dim = (curr_bucket, nom, y_vel)\n",
    "            vel_y_denom = curr_fold[k][1][1] # dim = (curr_bucket, denom, y_vel)\n",
    "\n",
    "            XY_FVAF = compute_XY_FVAF(vel_x_nom,vel_x_denom,vel_y_nom,vel_y_denom)\n",
    "            curr_fold_XY_FVAFs.append(XY_FVAF)\n",
    "\n",
    "        locals()[XY_FVAFs].append(curr_fold_XY_FVAFs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[0.3386737179256073, 0.25980955960403607],\n",
       " [0.3280304777878995, 0.218101291696028],\n",
       " [0.31791993941680063, 0.279729433204163],\n",
       " [0.2705727645254551, 0.23530381958004198],\n",
       " [0.2972852429907651, 0.23191511994156255],\n",
       " [0.35858105911466065, 0.2519671485653425],\n",
       " [0.2571985780195857, 0.1818187942326448],\n",
       " [0.3704386117069062, 0.27776864433615767],\n",
       " [0.3205895418018442, 0.22246253351950163],\n",
       " [0.3349608508140536, 0.23074439990865803]]"
      ]
     },
     "execution_count": 258,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "polarity_specific_XY_FVAFs_AD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving results in a matlab file \n",
    "from scipy.io import savemat\n",
    "\n",
    "# result1 = [polarity_specific_XY_FVAFs_AD, polarity_specific_test_XY_FVAFs]\n",
    "result1 = [polarity_specific_XY_FVAFs_AD, polarity_specific_XY_FVAFs_Rand]\n",
    "FrameStack1 = np.empty((2,), dtype=object)\n",
    "\n",
    "for i in range(len(result1)):\n",
    "    FrameStack1[i] = result1[i]\n",
    "\n",
    "savemat(\"Polarity_Specific_Buckets_Adjusted.mat\", {\"Polarity_Specific_XY_FVAFs_Adjusted\":FrameStack1})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "operands could not be broadcast together with shapes (85,1) (80,1) ",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m/Users/rbhatt6/Documents/MATLAB/Neural_Decoding_Dev/Examples_hippocampus/Cross_Bucket_New_Tests.ipynb Cell 23'\u001b[0m in \u001b[0;36m<cell line: 8>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/rbhatt6/Documents/MATLAB/Neural_Decoding_Dev/Examples_hippocampus/Cross_Bucket_New_Tests.ipynb#Y101sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m X_test \u001b[39m=\u001b[39m final_X_test_AD[cross]\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/rbhatt6/Documents/MATLAB/Neural_Decoding_Dev/Examples_hippocampus/Cross_Bucket_New_Tests.ipynb#Y101sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m y_test \u001b[39m=\u001b[39m final_y_test_AD[cross]\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/rbhatt6/Documents/MATLAB/Neural_Decoding_Dev/Examples_hippocampus/Cross_Bucket_New_Tests.ipynb#Y101sZmlsZQ%3D%3D?line=11'>12</a>\u001b[0m polarity_specific_test_XY_FVAFs\u001b[39m.\u001b[39mappend(run_model_kf_polarity_test(curr_models, X_test, y_test))\n",
      "File \u001b[0;32m~/Documents/MATLAB/Neural_Decoding_Dev/Neural_Decoding/runModelsKF.py:256\u001b[0m, in \u001b[0;36mrun_model_kf_polarity_test\u001b[0;34m(models, X_test, y_test)\u001b[0m\n\u001b[1;32m    253\u001b[0m new_y_test \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mconcatenate(new_y_test_list, axis\u001b[39m=\u001b[39m\u001b[39m0\u001b[39m)\n\u001b[1;32m    255\u001b[0m \u001b[39m#Get predictions\u001b[39;00m\n\u001b[0;32m--> 256\u001b[0m y_test_predicted \u001b[39m=\u001b[39m models[m]\u001b[39m.\u001b[39;49mpredict(new_X_test, new_y_test)\n\u001b[1;32m    257\u001b[0m R2_kf \u001b[39m=\u001b[39m get_R2_parts(new_y_test, y_test_predicted)\n\u001b[1;32m    259\u001b[0m \u001b[39m# Compute combined XY_FVAF\u001b[39;00m\n",
      "File \u001b[0;32m~/Documents/MATLAB/Neural_Decoding_Dev/Neural_Decoding/decoders.py:309\u001b[0m, in \u001b[0;36mKalmanFilterRegression.predict\u001b[0;34m(self, X_kf_test, y_test)\u001b[0m\n\u001b[1;32m    307\u001b[0m     K\u001b[39m=\u001b[39mP_m\u001b[39m*\u001b[39mH\u001b[39m.\u001b[39mT\u001b[39m*\u001b[39minv(H\u001b[39m*\u001b[39mP_m\u001b[39m*\u001b[39mH\u001b[39m.\u001b[39mT\u001b[39m+\u001b[39mQ) \u001b[39m#Calculate Kalman gain\u001b[39;00m\n\u001b[1;32m    308\u001b[0m     P\u001b[39m=\u001b[39m(np\u001b[39m.\u001b[39mmatrix(np\u001b[39m.\u001b[39meye(num_states))\u001b[39m-\u001b[39mK\u001b[39m*\u001b[39mH)\u001b[39m*\u001b[39mP_m\n\u001b[0;32m--> 309\u001b[0m     state\u001b[39m=\u001b[39mstate_m\u001b[39m+\u001b[39mK\u001b[39m*\u001b[39m(Z[:,t\u001b[39m+\u001b[39;49m\u001b[39m1\u001b[39;49m]\u001b[39m-\u001b[39;49mH\u001b[39m*\u001b[39;49mstate_m)\n\u001b[1;32m    310\u001b[0m     states[:,t\u001b[39m+\u001b[39m\u001b[39m1\u001b[39m]\u001b[39m=\u001b[39mnp\u001b[39m.\u001b[39msqueeze(state) \u001b[39m#Record state at the timestep\u001b[39;00m\n\u001b[1;32m    311\u001b[0m y_test_predicted\u001b[39m=\u001b[39mstates\u001b[39m.\u001b[39mT\n",
      "\u001b[0;31mValueError\u001b[0m: operands could not be broadcast together with shapes (85,1) (80,1) "
     ]
    }
   ],
   "source": [
    "# Trained Models for Specific Polarities are Now Separately Tested\n",
    "#(i.e. testing models trained on Random Fragments on AD fragments)\n",
    "from Neural_Decoding.runModelsKF import run_model_kf_polarity_test\n",
    "\n",
    "# Creating a local variable to hold predicted outputs for each frag type AND their trained models\n",
    "polarity_specific_test_XY_FVAFs = []\n",
    "\n",
    "for cross in range(0,10):\n",
    "    curr_models = polarity_specific_models_Rand[cross]\n",
    "    X_test = final_X_test_AD[cross]\n",
    "    y_test = final_y_test_AD[cross]\n",
    "    polarity_specific_test_XY_FVAFs.append(run_model_kf_polarity_test(curr_models, X_test, y_test))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 148,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "polarity_specific_test_XY_FVAFs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training and Testing Models on Same Bucket"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# (NEW) Within-Bucket Test with 10-fold Cross-Validation \n",
    "from Neural_Decoding.runModelsKF import run_model_kf_cv\n",
    "\n",
    "for i in range(len(frag_type)):\n",
    "\n",
    "    # Pulling local variables \n",
    "    frag_X_train =  locals()[\"final_X_train_\" + frag_type[i]]\n",
    "    frag_y_train = locals()[\"final_y_train_\" + frag_type[i]]\n",
    "    frag_X_test = locals()[\"final_X_test_\" + frag_type[i]]\n",
    "    frag_y_test = locals()[\"final_y_test_\" + frag_type[i]]\n",
    "\n",
    "    # Creating a local variable to hold predicted outputs for each frag type AND their trained models\n",
    "    parts = \"pred_parts_\" + frag_type[i]\n",
    "    models = \"models_\" + frag_type[i]\n",
    "    locals()[parts] = []\n",
    "    locals()[models] = []\n",
    "\n",
    "    for cross in range(0,10):\n",
    "        X_train = frag_X_train[cross]\n",
    "        y_train = frag_y_train[cross]\n",
    "        X_test = frag_X_test[cross]\n",
    "        y_test = frag_y_test[cross]\n",
    "        \n",
    "        curr_R2s, curr_models = run_model_kf_cv(X_train, y_train, X_test,y_test, \"parts\")\n",
    "        locals()[parts].append(curr_R2s)\n",
    "        locals()[models].append(curr_models)\n",
    "\n",
    "\n",
    "# len(within_bucket_R2s) # 10\n",
    "# len(within_bucket_R2s[0]) # 16\n",
    "# within_bucket_R2s[0][0] # each fold has R2s (vel,pos,acc) for each of the 16 buckets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "# (NEW) Compute Within-Bucket combined XY_FVAF \n",
    "# For velocity only, although all kinematic variables should be used for model-fitting and predicted for.\n",
    "from Neural_Decoding.metrics import compute_XY_FVAF\n",
    "\n",
    "# 'pred_parts_AD' was thankfully also a list of arrays containing the R2s\n",
    "\n",
    "for i in range(len(frag_type)):\n",
    "\n",
    "    # Pulling local variables (computed R2s for parts)\n",
    "    R2s_parts = locals()[\"pred_parts_\" + frag_type[i]]\n",
    "\n",
    "    # Creating a local variable to hold final XY_FVAFs\n",
    "    XY_FVAFs = \"XY_FVAFs_\" + frag_type[i]\n",
    "    locals()[XY_FVAFs] = []\n",
    "\n",
    "    for j in range(0,10):\n",
    "        curr_fold = R2s_parts[j]\n",
    "\n",
    "        curr_fold_XY_FVAFs = []\n",
    "        for k in range(0,16):\n",
    "            #curr_bucket = Kalman_R2s_combined[i]\n",
    "            vel_x_nom = curr_fold[k][0][0] # dim = (curr_bucket, nom, x_vel)\n",
    "            vel_x_denom = curr_fold[k][1][0] # dim = (curr_bucket, denom, x_vel)\n",
    "            vel_y_nom = curr_fold[k][0][1] # dim = (curr_bucket, nom, y_vel)\n",
    "            vel_y_denom = curr_fold[k][1][1] # dim = (curr_bucket, denom, y_vel)\n",
    "\n",
    "            XY_FVAF = compute_XY_FVAF(vel_x_nom,vel_x_denom,vel_y_nom,vel_y_denom)\n",
    "            curr_fold_XY_FVAFs.append(XY_FVAF)\n",
    "\n",
    "        locals()[XY_FVAFs].append(curr_fold_XY_FVAFs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(XY_FVAFs_Rand)\n",
    "len(XY_FVAFs_Rand[0])\n",
    "\n",
    "XY_FVAFs_AD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving results in a matlab file \n",
    "from scipy.io import savemat\n",
    "\n",
    "result1 = [XY_FVAFs_AD, XY_FVAFs_Rand]\n",
    "\n",
    "FrameStack1 = np.empty((2,), dtype=object)\n",
    "\n",
    "for i in range(len(result1)):\n",
    "    FrameStack1[i] = result1[i]\n",
    "\n",
    "savemat(\"Within_Bucket.mat\", {\"Within_Bucket_XY_FVAFs\":FrameStack1})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cross-Bucket Tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "# (NEW) Opposite-polarity, Opposite-direction, Complete-opposite and Full cross-bucket tests\n",
    "from Neural_Decoding.runModelsKF import opposite_polarity_test_cv, complete_opposite_bucket_test_cv, opposite_direction_test_cv, cross_buckets_test_cv\n",
    "\n",
    "for i in range(len(frag_type)):\n",
    "\n",
    "    # Pulling local variables \n",
    "    frag_X_test = locals()[\"final_X_test_\" + frag_type[i]]\n",
    "    frag_y_test = locals()[\"final_y_test_\" + frag_type[i]]\n",
    "    models = locals()[\"models_\" + frag_type[i]]\n",
    "\n",
    "    # Creating a local variable to hold FVAFs\n",
    "    opposite_polarity = \"opposite_polarity_FVAFs_\" + frag_type[i]\n",
    "    opposite_direction = \"opposite_direction_FVAFs_\" + frag_type[i]\n",
    "    complete_opposite = \"complete_opposite_FVAFs_\" + frag_type[i]\n",
    "    #cross_buckets = \"cross_buckets_FVAFs_\" + frag_type[i]\n",
    "    locals()[opposite_polarity] = []\n",
    "    locals()[opposite_direction] = []\n",
    "    locals()[complete_opposite] = []\n",
    "    #locals()[cross_buckets] = []\n",
    "\n",
    "\n",
    "    for j in range(0,10):\n",
    "        curr_models = models[j]\n",
    "        curr_X_test = frag_X_test[j]\n",
    "        curr_y_test = frag_y_test[j]\n",
    "\n",
    "        # Creating a local variable to hold opposite-polarity and complete-opposite FVAFs\n",
    "        locals()[opposite_polarity].append(opposite_polarity_test_cv(curr_models, curr_X_test, curr_y_test, \"parts\", frag_type[i]))\n",
    "        locals()[opposite_direction].append(opposite_direction_test_cv(curr_models, curr_X_test, curr_y_test, \"parts\", frag_type[i]))\n",
    "        locals()[complete_opposite].append(complete_opposite_bucket_test_cv(curr_models, curr_X_test, curr_y_test, \"parts\", frag_type[i]))\n",
    "        #locals()[cross_buckets].append(cross_buckets_test_cv(curr_models, curr_X_test, curr_y_test, \"parts\"))\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "opposite_polarity_FVAFs_AD\n",
    "opposite_direction_FVAFs_AD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving results in a matlab file \n",
    "from scipy.io import savemat\n",
    "\n",
    "result1 = [opposite_polarity_FVAFs_AD, opposite_polarity_FVAFs_Rand]\n",
    "result2 = [opposite_direction_FVAFs_AD, opposite_direction_FVAFs_Rand]\n",
    "result3 = [complete_opposite_FVAFs_AD, complete_opposite_FVAFs_Rand]\n",
    "\n",
    "FrameStack1 = np.empty((2,), dtype=object)\n",
    "FrameStack2 = np.empty((2,), dtype=object)\n",
    "FrameStack3 = np.empty((2,), dtype=object)\n",
    "for i in range(len(result1)):\n",
    "    FrameStack1[i] = result1[i]\n",
    "    FrameStack2[i] = result2[i]\n",
    "    FrameStack3[i] = result3[i]\n",
    "savemat(\"Opposite_Polarity.mat\", {\"Opposite_Polarity\":FrameStack1})\n",
    "savemat(\"Opposite_Direction.mat\", {\"Opposite_Direction\":FrameStack2})\n",
    "savemat(\"Complete_Opposite.mat\", {\"Complete_Opposite\":FrameStack3})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part D: Partitioning and Running the Kalman Filter on Separate Training and Test Buckets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part E: Save all outputs (i.e. FVAFs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving results in a matlab file \n",
    "from scipy.io import savemat\n",
    "\n",
    "result1 = [XY_FVAFs_AD, XY_FVAFs_Rand]\n",
    "result2 = [cross_buckets_FVAFs_AD, cross_buckets_FVAFs_Rand]\n",
    "result3 = [opposite_polarity_FVAFs_AD, opposite_polarity_FVAFs_Rand]\n",
    "result4 = [opposite_direction_FVAFs_AD, opposite_direction_FVAFs_Rand]\n",
    "result5 = [complete_opposite_FVAFs_AD, complete_opposite_FVAFs_Rand]\n",
    "\n",
    "FrameStack1 = np.empty((2,), dtype=object)\n",
    "FrameStack2 = np.empty((2,), dtype=object)\n",
    "FrameStack3 = np.empty((2,), dtype=object)\n",
    "FrameStack4 = np.empty((2,), dtype=object)\n",
    "FrameStack5 = np.empty((2,), dtype=object)\n",
    "for i in range(len(result1)):\n",
    "    FrameStack1[i] = result1[i]\n",
    "    FrameStack2[i] = result2[i]\n",
    "    FrameStack3[i] = result3[i]\n",
    "    FrameStack4[i] = result4[i]\n",
    "    FrameStack5[i] = result5[i]\n",
    "savemat(\"Within_Bucket.mat\", {\"XY_FVAF\":FrameStack1})\n",
    "savemat(\"Cross_Bucket.mat\", {\"Cross_Bucket\":FrameStack2})\n",
    "savemat(\"Opposite_Polarity.mat\", {\"Opposite_Polarity\":FrameStack3})\n",
    "savemat(\"Opposite_Direction.mat\", {\"Opposite_Direction\":FrameStack4})\n",
    "savemat(\"Complete_Opposite.mat\", {\"Complete_Opposite\":FrameStack5})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Notes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(sortIn_AD)):\n",
    "    X_kf = sortIn_AD[i]\n",
    "    y_kf = decoder_output_AD[i]\n",
    "    num_examples_kf=X_kf.shape[0] # nRows (b/c nCols = number of units)\n",
    "    \n",
    "    #Note that each range has a buffer of 1 bin at the beginning and end\n",
    "    #This makes it so that the different sets don't include overlapping data\n",
    "    training_set=np.arange(int(np.round(training_range[0]*num_examples_kf))+1,int(np.round(training_range[1]*num_examples_kf))-1)\n",
    "    testing_set=np.arange(int(np.round(testing_range[0]*num_examples_kf))+1,int(np.round(testing_range[1]*num_examples_kf))-1)\n",
    "    valid_set=np.arange(int(np.round(valid_range[0]*num_examples_kf))+1,int(np.round(valid_range[1]*num_examples_kf))-1)\n",
    "\n",
    "    #Get training data\n",
    "    X_kf_train=X_kf[training_set,:]\n",
    "    y_kf_train=y_kf[training_set,:]\n",
    "\n",
    "    #Get testing data\n",
    "    X_kf_test=X_kf[testing_set,:]\n",
    "    y_kf_test=y_kf[testing_set,:]\n",
    "\n",
    "    #Get validation data\n",
    "    X_kf_valid=X_kf[valid_set,:]\n",
    "    y_kf_valid=y_kf[valid_set,:]\n",
    "\n",
    "    #Z-score inputs \n",
    "    X_kf_train_mean=np.nanmean(X_kf_train,axis=0)\n",
    "    X_kf_train_std=np.nanstd(X_kf_train,axis=0)\n",
    "    X_kf_train=(X_kf_train-X_kf_train_mean)/X_kf_train_std\n",
    "    X_kf_test=(X_kf_test-X_kf_train_mean)/X_kf_train_std\n",
    "    X_kf_valid=(X_kf_valid-X_kf_train_mean)/X_kf_train_std\n",
    "\n",
    "    #Zero-center outputs\n",
    "    y_kf_train_mean=np.mean(y_kf_train,axis=0)\n",
    "    y_kf_train=y_kf_train-y_kf_train_mean\n",
    "    y_kf_test=y_kf_test-y_kf_train_mean\n",
    "    y_kf_valid=y_kf_valid-y_kf_train_mean\n",
    "\n",
    "    #Declare model\n",
    "    model_kf=KalmanFilterDecoder(C=1) #There is one optional parameter (see ReadMe)\n",
    "\n",
    "    #Fit model\n",
    "    model_kf.fit(X_kf_train,y_kf_train)\n",
    "\n",
    "    #Get predictions\n",
    "    y_valid_predicted_kf=model_kf.predict(X_kf_valid,y_kf_valid)\n",
    "\n",
    "    #Get metrics of fit (see read me for more details on the differences between metrics)\n",
    "    #First I'll get the R^2\n",
    "    R2_kf=get_R2(y_kf_valid,y_valid_predicted_kf)\n",
    "    print('R2:',R2_kf[0:2]) #I'm just printing the R^2's of the 1st and 2nd entries that correspond to the positions\n",
    "    #Next I'll get the rho^2 (the pearson correlation squared)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Only using velocity \n",
    "## Format Kinematics Data (outputs) ##\n",
    "# For the Kalman filter, we use the position, velocity, and acceleration as outputs.\n",
    "# Ultimately, we are only concerned with the goodness of fit of velocity, but using them all as covariates helps performance.\n",
    "\n",
    "for i in range(len(frag_type)):\n",
    "    # Pulling local variables into new, temp variables\n",
    "    output = locals()[\"sortOut_\"+frag_type[i]]\n",
    "    \n",
    "    # Creating a local variable for each decoder_output\n",
    "    decoder_output = \"decoder_output_\" + frag_type[i]\n",
    "    locals()[decoder_output] = []\n",
    "\n",
    "    for j in range(len(output)): # Number of buckets (i.e. 16 or 8)\n",
    "        nFrags = output[j][0].shape[0]\n",
    "        temp2 = []\n",
    "        for k in range(0, nFrags, 1): # Number of fragments #output[0][0].shape[0]\n",
    "            vel_X = float(output[j][0][k])\n",
    "            vel_Y = float(output[j][1][k])\n",
    "            temp = [vel_X, vel_Y]\n",
    "            temp2.append(np.array(temp))\n",
    "        locals()[decoder_output].append(np.array(temp2))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example Code for Concatenating Arrays from a List of Arrays\n",
    "\n",
    "listArrays = final_y_test_AD[0][0:3]\n",
    "listArrays\n",
    "\n",
    "sampleList = [1,2]\n",
    "\n",
    "slipList = [listArrays[i] for i in sampleList]\n",
    "slip = np.concatenate(slipList, axis=0)\n",
    "\n",
    "len(slipList)\n",
    "slip.shape"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
  },
  "kernelspec": {
   "display_name": "Python 3.8.9 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
