{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cross-Bucket Tests on all 3 Fragment Types using Kalman Filter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part A: (1) Import Fragment Data from MATLAB, (2) Save Them in a .pickle file, and (3) Load Them In\n",
    "\n",
    "###### (HOWEVER, YOU DON'T NEED TO LOAD THEM IN AGAIN SINCE STEP 1 ALREADY DOES THAT FOR YOU!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "## (Configuration) Allows you to return multiple variables from a single cell ##\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = 'all'\n",
    "\n",
    "## Allows you to import files from another folder in current directory ## \n",
    "import os \n",
    "import sys \n",
    "module_path = os.path.abspath(os.path.join('..'))\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path)\n",
    "\n",
    "## Import standard package ###\n",
    "import numpy as np\n",
    "from scipy import io\n",
    "import sys\n",
    "\n",
    "# Specify Fragment Types to be used for this Anaylsis \n",
    "frag_type = ['AD', 'HV', 'VM'] # ['AccDec', 'HillValley', 'VelMin']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    " ## (OLD) Import Data ## \n",
    "# We'll load in position data and derive velocity and acceleration from it \n",
    "\n",
    "folder = '/Users/rbhatt6/Documents/MATLAB/' # For Windows: folder='C:\\\\Users\\\\rbhatt1\\\\Downloads\\\\' \n",
    "\n",
    " #locals()[\"sortIn\"+frag_type[i]] = io.loadmat(folder+'cleanedSortIn.mat')\n",
    "for i in range(len(frag_type)):\n",
    "    input = \"sortIn_\"+frag_type[i]\n",
    "    locals()[input] = io.loadmat(folder + input + '.mat')\n",
    "   \n",
    "    outputX = \"sortX_\"+frag_type[i]\n",
    "    locals()[outputX] = io.loadmat(folder + outputX + '.mat')\n",
    "\n",
    "    outputY = \"sortY_\"+frag_type[i]\n",
    "    locals()[outputY] = io.loadmat(folder + outputY + '.mat')\n",
    "\n",
    "    locals()[input] = np.squeeze(list(locals()[input].values())[3])\n",
    "    locals()[outputX] = np.squeeze(list(locals()[outputX].values())[3])\n",
    "    locals()[outputY] = np.squeeze(list(locals()[outputY].values())[3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "## (OLD) Save Data in .pickle files ## \n",
    "import pickle\n",
    "\n",
    "data_folder='/Users/rbhatt6/Documents/MATLAB/' # Folder where you want to save the data\n",
    "\n",
    "for i in range(len(frag_type)):\n",
    "    input = \"sortIn_\"+frag_type[i]\n",
    "    outputX = \"sortX_\"+frag_type[i]\n",
    "    outputY = \"sortY_\"+frag_type[i]\n",
    "\n",
    "    with open(data_folder + input + '.pickle','wb') as f:\n",
    "        pickle.dump(locals()[input],f)\n",
    "\n",
    "    with open(data_folder + outputX + '.pickle','wb') as f:\n",
    "        pickle.dump(locals()[outputX],f)\n",
    "\n",
    "    with open(data_folder + outputY + '.pickle','wb') as f:\n",
    "        pickle.dump(locals()[outputY],f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part B: Preprocessing Decoder Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import standard packages\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "from scipy import io\n",
    "from scipy import stats\n",
    "import pickle\n",
    "import sys\n",
    "\n",
    "#Import metrics\n",
    "from Neural_Decoding.metrics import get_R2\n",
    "from Neural_Decoding.metrics import get_rho\n",
    "from Neural_Decoding.metrics import get_R2_parts\n",
    "\n",
    "#Import decoder functions\n",
    "from Neural_Decoding.decoders import KalmanFilterDecoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "#For the Kalman filter, we use the position, velocity, and acceleration as outputs.\n",
    "#Ultimately, we are only concerned with the goodness of fit of velocity, but using them all as covariates helps performance.\n",
    "\n",
    "for i in range(len(frag_type)):\n",
    "    \n",
    "    # Pulling local variables into new, temp variables\n",
    "    outputX = locals()[\"sortX_\"+frag_type[i]]\n",
    "    outputY = locals()[\"sortY_\"+frag_type[i]]\n",
    "    \n",
    "    # Creating a local variable for each decoder_output\n",
    "    decoder_output = \"decoder_output_\" + frag_type[i]\n",
    "    locals()[decoder_output] = []\n",
    "\n",
    "    for j in range(len(outputX)):\n",
    "        # Determine X and Y velocities \n",
    "        vel_X = (np.diff(outputX[j], axis = 0))      \n",
    "        vel_X = np.concatenate((vel_X,vel_X[-1:,:]),axis=0)\n",
    "        vel_Y = (np.diff(outputY[j], axis = 0))      \n",
    "        vel_Y = np.concatenate((vel_Y,vel_Y[-1:,:]),axis=0)\n",
    "\n",
    "        # Determine X and Y accelerations \n",
    "        acc_X = (np.diff(vel_X, axis = 0))   \n",
    "        acc_X = np.concatenate((acc_X,acc_X[-1:,:]),axis=0)\n",
    "        acc_Y = (np.diff(vel_Y, axis = 0))      \n",
    "        acc_Y = np.concatenate((acc_Y,acc_Y[-1:,:]),axis=0)\n",
    "\n",
    "        # Concatenating all 3 kinematics for X and Y components\n",
    "        # Corrected decoder outputs (format = x,y vel, x,y acc, x,y pos)\n",
    "        temp = np.array(np.concatenate((vel_X, vel_Y, acc_X, acc_Y, outputX[j], outputY[j]),axis=1))\n",
    "\n",
    "        locals()[decoder_output].append(temp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1633, 6)"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "array([  7.03923131,  -7.73396546,   0.        ,   0.        ,\n",
       "        24.60974452, 242.11359228])"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "decoder_output_AD[0].shape\n",
    "decoder_output_AD[0][-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part C: Partitioning and Running the Kalman Filter on the Same Buckets "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Set what part of data should be part of the training/testing/validation sets\n",
    "\n",
    "training_range=[0, 0.8]\n",
    "valid_range=[0.8,0.9]\n",
    "testing_range=[0.9, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/rbhatt6/Documents/Neural_Decoding_Dev/Neural_Decoding/runModelsKF.py:48: RuntimeWarning: invalid value encountered in true_divide\n",
      "  X_train=(X_train-X_train_mean)/X_train_std\n",
      "/Users/rbhatt6/Documents/Neural_Decoding_Dev/Neural_Decoding/runModelsKF.py:49: RuntimeWarning: divide by zero encountered in true_divide\n",
      "  X_test=(X_test-X_train_mean)/X_train_std\n",
      "/Users/rbhatt6/Documents/Neural_Decoding_Dev/Neural_Decoding/runModelsKF.py:49: RuntimeWarning: invalid value encountered in true_divide\n",
      "  X_test=(X_test-X_train_mean)/X_train_std\n",
      "/Users/rbhatt6/Documents/Neural_Decoding_Dev/Neural_Decoding/runModelsKF.py:50: RuntimeWarning: invalid value encountered in true_divide\n",
      "  X_valid=(X_valid-X_train_mean)/X_train_std\n"
     ]
    }
   ],
   "source": [
    "# Running decoder on X and Y components together to get the prediction nom and denom to later compute the combined XY_FVAF\n",
    "from Neural_Decoding.runModelsKF import run_model_kf\n",
    "\n",
    "for i in range(len(frag_type)):\n",
    "\n",
    "    # Pulling local variables \n",
    "    input = locals()[\"sortIn_\"+frag_type[i]]\n",
    "    output = locals()[\"decoder_output_\"+frag_type[i]]\n",
    "\n",
    "    # Creating a local variable to hold (X, Y) predicted outputs for each frag type \n",
    "    parts = \"pred_parts_\" + frag_type[i]\n",
    "    locals()[parts] = run_model_kf(input, output, training_range, testing_range, valid_range, \"parts\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "105691.96777379638"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred_parts_AD[0][0][5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute combined XY_FVAF (velocity only)\n",
    "from Neural_Decoding.metrics import compute_XY_FVAF\n",
    "\n",
    "for i in range(len(frag_type)):\n",
    "\n",
    "    # Pulling previously computed predicted_parts (i.e. nom and denom)\n",
    "    parts = locals()[ \"pred_parts_\" + frag_type[i]]\n",
    "    \n",
    "    # Creating a local variable to hold XY_FVAFs each frag type \n",
    "    XY_FVAF = \"XY_FVAF_\" + frag_type[i]\n",
    "    locals()[XY_FVAF] = []\n",
    "\n",
    "    for j in range(len(parts)):\n",
    "        #curr_bucket = Kalman_R2s_combined[i]\n",
    "        vel_x_nom = parts[j][0][0] # dim = (curr_bucket, nom, x_vel)\n",
    "        vel_x_denom = parts[j][1][0] # dim = (curr_bucket, denom, x_vel)\n",
    "        vel_y_nom = parts[j][0][1] # dim = (curr_bucket, nom, y_vel)\n",
    "        vel_y_denom = parts[j][1][1] # dim = (curr_bucket, denom, y_vel)\n",
    "\n",
    "        curr_bucket_XY_FVAF = compute_XY_FVAF(vel_x_nom,vel_x_denom,vel_y_nom,vel_y_denom)\n",
    "        locals()[XY_FVAF].append(curr_bucket_XY_FVAF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.1841062251340041,\n",
       " nan,\n",
       " 0.15763774570315126,\n",
       " 0.14912740243211653,\n",
       " 0.1498301473946998,\n",
       " 0.1878203662639506,\n",
       " 0.15412887524048502,\n",
       " 0.17529831111052663,\n",
       " 0.07017912624038569,\n",
       " 0.0896783076780211,\n",
       " -0.04286038172450968,\n",
       " -0.029913522789741798,\n",
       " 0.04915769389920999,\n",
       " 0.06663389571021072,\n",
       " 0.06463894052602315,\n",
       " -0.09471064487266201]"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "XY_FVAF_HV"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part D: Partitioning and Running the Kalman Filter on Separate Training and Test Buckets"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
  },
  "kernelspec": {
   "display_name": "Python 3.8.9 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
