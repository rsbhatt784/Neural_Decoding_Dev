{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cross-Bucket Tests on all 3 Fragment Types using Kalman Filter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part A: (1) Import Fragment Data from MATLAB, (2) Save Them in a .pickle file, and (3) Load Them In\n",
    "\n",
    "###### (HOWEVER, YOU DON'T NEED TO LOAD THEM IN AGAIN SINCE STEP 1 ALREADY DOES THAT FOR YOU!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "## (Configuration) Allows you to return multiple variables from a single cell ##\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = 'all'\n",
    "\n",
    "## Allows you to import files from another folder in current directory ## \n",
    "import os \n",
    "import sys \n",
    "module_path = os.path.abspath(os.path.join('..'))\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path)\n",
    "\n",
    "## Import standard package ###\n",
    "import numpy as np\n",
    "from scipy import io\n",
    "import sys\n",
    "\n",
    "# Specify Fragment Types to be used for this Anaylsis \n",
    "frag_type = ['AD', 'HV', 'VM'] # ['AccDec', 'HillValley', 'VelMin']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "## (NEW) Import Data ##\n",
    "# We'll load in position data and derive velocity and acceleration from it \n",
    "\n",
    "folder = '/Users/rbhatt6/Documents/MATLAB/' # For Windows: folder='C:\\\\Users\\\\rbhatt1\\\\Downloads\\\\' \n",
    "\n",
    " #locals()[\"sortIn\"+frag_type[i]] = io.loadmat(folder+'cleanedSortIn.mat')\n",
    "for i in range(len(frag_type)):\n",
    "    input = \"sortIn_\"+frag_type[i]\n",
    "    locals()[input] = io.loadmat(folder + input + '.mat')\n",
    "\n",
    "    output = \"sortOut_\" + frag_type[i]\n",
    "    locals()[output] = io.loadmat(folder + output + '.mat')\n",
    "\n",
    "    locals()[input] = np.squeeze(list(locals()[input].values())[3])\n",
    "    locals()[output] = np.squeeze(list(locals()[output].values())[3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part B: Preprocessing Decoder Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "WARNING: statsmodels is not installed. You will be unable to use the Naive Bayes Decoder\n",
      "\n",
      "WARNING: Xgboost package is not installed. You will be unable to use the xgboost decoder\n",
      "\n",
      "WARNING: Keras package is not installed. You will be unable to use all neural net decoders\n"
     ]
    }
   ],
   "source": [
    "#Import standard packages\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "from scipy import io\n",
    "from scipy import stats\n",
    "import pickle\n",
    "import sys\n",
    "\n",
    "#Import metrics\n",
    "from Neural_Decoding.metrics import get_R2\n",
    "from Neural_Decoding.metrics import get_rho\n",
    "from Neural_Decoding.metrics import get_R2_parts\n",
    "\n",
    "#Import decoder functions\n",
    "from Neural_Decoding.decoders import KalmanFilterDecoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# (NEW)\n",
    "#For the Kalman filter, we use the position, velocity, and acceleration as outputs.\n",
    "#Ultimately, we are only concerned with the goodness of fit of velocity, but using them all as covariates helps performance.\n",
    "\n",
    "for i in range(len(frag_type)):\n",
    "    # Pulling local variables into new, temp variables\n",
    "    output = locals()[\"sortOut_\"+frag_type[i]]\n",
    "    \n",
    "    # Creating a local variable for each decoder_output\n",
    "    decoder_output = \"decoder_output_\" + frag_type[i]\n",
    "    locals()[decoder_output] = []\n",
    "\n",
    "    for j in range(len(output)): # Number of buckets (i.e. 16 or 8)\n",
    "        nFrags = output[j][0].shape[0]\n",
    "        temp2 = []\n",
    "        for k in range(0, nFrags, 1): # Number of fragments #output[0][0].shape[0]\n",
    "            vel_X = float(output[j][0][k])\n",
    "            vel_Y = float(output[j][1][k])\n",
    "            acc_X = float(output[j][2][k])\n",
    "            acc_Y = float(output[j][3][k])\n",
    "            pos_X = float(output[j][4][k])\n",
    "            pos_Y = float(output[j][5][k])\n",
    "            temp = [vel_X, vel_Y, acc_X, acc_Y, pos_X, pos_Y]\n",
    "            temp2.append(np.array(temp))\n",
    "            #locals()[decoder_output][j][k].append(np.array(temp))\n",
    "        #locals()[decoder_output][j].append(np.array(temp2))\n",
    "        locals()[decoder_output].append(np.array(temp2))\n",
    "        #temp = np.array(np.concatenate((vel_X, vel_Y, acc_X, acc_Y, outputX[j], outputY[j]),axis=1))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part C: Partitioning and Running the Kalman Filter on the Same Buckets "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Set what part of data should be part of the training/testing/validation sets\n",
    "\n",
    "training_range=[0, 0.8]\n",
    "valid_range=[0.8,0.9]\n",
    "testing_range=[0.9, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Running decoder on X and Y components together to get the prediction nom and denom to later compute the combined XY_FVAF\n",
    "from Neural_Decoding.runModelsKF import run_model_kf\n",
    "\n",
    "for i in range(len(frag_type)):\n",
    "\n",
    "    # Pulling local variables \n",
    "    input = locals()[\"sortIn_\"+frag_type[i]]\n",
    "    output = locals()[\"decoder_output_\"+frag_type[i]]\n",
    "\n",
    "    # Creating a local variable to hold (X, Y) predicted outputs for each frag type AND their trained models\n",
    "    parts = \"pred_parts_\" + frag_type[i]\n",
    "    models = \"models_\" + frag_type[i]\n",
    "    #locals()[parts] = run_model_kf(input, output, training_range, testing_range, valid_range, \"parts\", \"within_bucket\")\n",
    "    R2s, trained_models = run_model_kf(input, output, training_range, testing_range, valid_range, \"parts\")\n",
    "    locals()[parts] = R2s\n",
    "    locals()[models] = trained_models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute combined XY_FVAF (velocity only)\n",
    "from Neural_Decoding.metrics import compute_XY_FVAF\n",
    "\n",
    "for i in range(len(frag_type)):\n",
    "\n",
    "    # Pulling previously computed predicted_parts (i.e. nom and denom)\n",
    "    parts = locals()[ \"pred_parts_\" + frag_type[i]]\n",
    "    \n",
    "    # Creating a local variable to hold XY_FVAFs each frag type \n",
    "    XY_FVAF = \"XY_FVAF_\" + frag_type[i]\n",
    "    locals()[XY_FVAF] = []\n",
    "\n",
    "    for j in range(len(parts)):\n",
    "        #curr_bucket = Kalman_R2s_combined[i]\n",
    "        vel_x_nom = parts[j][0][0] # dim = (curr_bucket, nom, x_vel)\n",
    "        vel_x_denom = parts[j][1][0] # dim = (curr_bucket, denom, x_vel)\n",
    "        vel_y_nom = parts[j][0][1] # dim = (curr_bucket, nom, y_vel)\n",
    "        vel_y_denom = parts[j][1][1] # dim = (curr_bucket, denom, y_vel)\n",
    "\n",
    "        curr_bucket_XY_FVAF = compute_XY_FVAF(vel_x_nom,vel_x_denom,vel_y_nom,vel_y_denom)\n",
    "        locals()[XY_FVAF].append(curr_bucket_XY_FVAF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.274861529126343,\n",
       " 0.17285629320953566,\n",
       " 0.3689220685199799,\n",
       " 0.359915096119927,\n",
       " 0.4215025371900333,\n",
       " 0.37625623031487165,\n",
       " 0.4259858030424114,\n",
       " 0.3695129756493417,\n",
       " 0.18764753306435566,\n",
       " 0.20314343596785345,\n",
       " 0.18883410343142448,\n",
       " 0.0813192122389158,\n",
       " 0.07460811664925293,\n",
       " 0.14498036696841665,\n",
       " 0.04125201098470266,\n",
       " 0.13951618425674284]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "XY_FVAF_AD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Testing cross-bucket function on AD fragments \n",
    "# pred_parts_ is actually the models now here\n",
    "from Neural_Decoding.runModelsKF import cross_polarity_test, complete_opposite_bucket_test\n",
    "\n",
    "ans3 = cross_polarity_test(pred_parts_AD, sortIn_AD, decoder_output_AD, training_range, testing_range, valid_range, \"parts\")\n",
    "ans4 = complete_opposite_bucket_test(models_VM, sortIn_VM, decoder_output_VM, training_range, testing_range, valid_range, \"parts\", \"VM\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Testing cross-bucket function on AD fragments \n",
    "# pred_parts_ is actually the models now here\n",
    "from Neural_Decoding.runModelsKF import cross_buckets_test\n",
    "\n",
    "ans, ans2 = cross_buckets_test(pred_parts_AD, sortIn_AD, decoder_output_AD, training_range, testing_range, valid_range, \"parts\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part D: Partitioning and Running the Kalman Filter on Separate Training and Test Buckets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cross-bucket test for all fragment types \n",
    "from Neural_Decoding.runModelsKF import cross_buckets_test\n",
    "\n",
    "for i in range(len(frag_type)):\n",
    "\n",
    "    # Pulling local variables \n",
    "    input = locals()[\"sortIn_\" + frag_type[i]]\n",
    "    output = locals()[\"decoder_output_\" + frag_type[i]]\n",
    "    models = locals()[\"models_\" + frag_type[i]]\n",
    "\n",
    "    # Creating a local variable to hold cross_bucket FVAFs (moreso, total-residual for each cross-bucket test)\n",
    "    cross_buckets = \"cross_buckets_FVAFs_\" + frag_type[i]\n",
    "    R2s, total_res = cross_buckets_test(models, input, output, training_range, testing_range, valid_range, \"parts\")\n",
    "    locals()[cross_buckets] = total_res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cross-polarity and Complete-opposite tests for all fragment types \n",
    "from Neural_Decoding.runModelsKF import cross_buckets_test, cross_polarity_test, complete_opposite_bucket_test\n",
    "\n",
    "for i in range(len(frag_type)):\n",
    "\n",
    "    # Pulling local variables \n",
    "    input = locals()[\"sortIn_\" + frag_type[i]]\n",
    "    output = locals()[\"decoder_output_\" + frag_type[i]]\n",
    "    models = locals()[\"models_\" + frag_type[i]]\n",
    "\n",
    "    # Creating a local variable to hold cross-polarity and complete-opposite FVAFs\n",
    "    cross_polarity = \"cross_polarity_FVAFs_\" + frag_type[i]\n",
    "    locals()[cross_polarity] = cross_polarity_test(models, input, output, training_range, testing_range, valid_range, \"parts\", frag_type[i])\n",
    "    complete_opposite = \"complete_opposite_FVAFs_\" + frag_type[i]\n",
    "    locals()[complete_opposite] = complete_opposite_bucket_test(models, input, output, training_range, testing_range, valid_range, \"parts\", frag_type[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part E: Save all outputs (i.e. FVAFs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving results in a matlab file \n",
    "from scipy.io import savemat\n",
    "\n",
    "# result1 = [XY_FVAF_AD, XY_FVAF_HV, XY_FVAF_VM]\n",
    "# result2 = [cross_buckets_FVAFs_AD, cross_buckets_FVAFs_HV, cross_buckets_FVAFs_VM]\n",
    "result3 = [cross_polarity_FVAFs_AD, cross_polarity_FVAFs_HV, cross_polarity_FVAFs_VM]\n",
    "result4 = [complete_opposite_FVAFs_AD, complete_opposite_FVAFs_HV, complete_opposite_FVAFs_VM]\n",
    "\n",
    "# FrameStack1 = np.empty((3,), dtype=object)\n",
    "# FrameStack2 = np.empty((3,), dtype=object)\n",
    "FrameStack3 = np.empty((3,), dtype=object)\n",
    "FrameStack4 = np.empty((3,), dtype=object)\n",
    "for i in range(len(result3)):\n",
    "    # FrameStack1[i] = result1[i]\n",
    "    # FrameStack2[i] = result2[i]\n",
    "    FrameStack3[i] = result3[i]\n",
    "    FrameStack4[i] = result4[i]\n",
    "# savemat(\"XY_FVAF_All_Frags.mat\", {\"XY_FVAF\":FrameStack1})\n",
    "# savemat(\"Cross_Bucket_All_Frags.mat\", {\"Cross_Bucket\":FrameStack2})\n",
    "savemat(\"Cross_Polarity_All_Frags.mat\", {\"Cross_Polarity\":FrameStack3})\n",
    "savemat(\"Complete_Opposite_All_Frags.mat\", {\"Complete_Opposite\":FrameStack4})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Notes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(sortIn_AD)):\n",
    "    X_kf = sortIn_AD[i]\n",
    "    y_kf = decoder_output_AD[i]\n",
    "    num_examples_kf=X_kf.shape[0] # nRows (b/c nCols = number of units)\n",
    "    \n",
    "    #Note that each range has a buffer of 1 bin at the beginning and end\n",
    "    #This makes it so that the different sets don't include overlapping data\n",
    "    training_set=np.arange(int(np.round(training_range[0]*num_examples_kf))+1,int(np.round(training_range[1]*num_examples_kf))-1)\n",
    "    testing_set=np.arange(int(np.round(testing_range[0]*num_examples_kf))+1,int(np.round(testing_range[1]*num_examples_kf))-1)\n",
    "    valid_set=np.arange(int(np.round(valid_range[0]*num_examples_kf))+1,int(np.round(valid_range[1]*num_examples_kf))-1)\n",
    "\n",
    "    #Get training data\n",
    "    X_kf_train=X_kf[training_set,:]\n",
    "    y_kf_train=y_kf[training_set,:]\n",
    "\n",
    "    #Get testing data\n",
    "    X_kf_test=X_kf[testing_set,:]\n",
    "    y_kf_test=y_kf[testing_set,:]\n",
    "\n",
    "    #Get validation data\n",
    "    X_kf_valid=X_kf[valid_set,:]\n",
    "    y_kf_valid=y_kf[valid_set,:]\n",
    "\n",
    "    #Z-score inputs \n",
    "    X_kf_train_mean=np.nanmean(X_kf_train,axis=0)\n",
    "    X_kf_train_std=np.nanstd(X_kf_train,axis=0)\n",
    "    X_kf_train=(X_kf_train-X_kf_train_mean)/X_kf_train_std\n",
    "    X_kf_test=(X_kf_test-X_kf_train_mean)/X_kf_train_std\n",
    "    X_kf_valid=(X_kf_valid-X_kf_train_mean)/X_kf_train_std\n",
    "\n",
    "    #Zero-center outputs\n",
    "    y_kf_train_mean=np.mean(y_kf_train,axis=0)\n",
    "    y_kf_train=y_kf_train-y_kf_train_mean\n",
    "    y_kf_test=y_kf_test-y_kf_train_mean\n",
    "    y_kf_valid=y_kf_valid-y_kf_train_mean\n",
    "\n",
    "    #Declare model\n",
    "    model_kf=KalmanFilterDecoder(C=1) #There is one optional parameter (see ReadMe)\n",
    "\n",
    "    #Fit model\n",
    "    model_kf.fit(X_kf_train,y_kf_train)\n",
    "\n",
    "    #Get predictions\n",
    "    y_valid_predicted_kf=model_kf.predict(X_kf_valid,y_kf_valid)\n",
    "\n",
    "    #Get metrics of fit (see read me for more details on the differences between metrics)\n",
    "    #First I'll get the R^2\n",
    "    R2_kf=get_R2(y_kf_valid,y_valid_predicted_kf)\n",
    "    print('R2:',R2_kf[0:2]) #I'm just printing the R^2's of the 1st and 2nd entries that correspond to the positions\n",
    "    #Next I'll get the rho^2 (the pearson correlation squared)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
  },
  "kernelspec": {
   "display_name": "Python 3.8.9 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
